{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9f63401",
   "metadata": {},
   "source": [
    "# Scraping Top Repositories from GitHub\n",
    "\n",
    "TODO (Intro):\n",
    "- Introduction about WebScraping\n",
    "- Introduction about the GitHub and the problem statement\n",
    "- Mention the tools you're using(Python, requests, Beautiful Soup, Pandas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7c6bbe",
   "metadata": {},
   "source": [
    "### Project Outline\n",
    "\n",
    "Here are the steps we follow\n",
    "- We're going to scrape https://github.com/topics\n",
    "- We'll get a list of topics. For each topic, we'll get topic title, topic page URL and topic description\n",
    "- For each topic, we'll get the top 25 repositories in the topic from the topic page\n",
    "- For each repository, we'll grab the repo name, username, stars and repo URL\n",
    "- For each topic we'll create a CSV file in the following format:\n",
    "Repo Name,Username,Stars,Repo URL\n",
    "three.js,mrdoob,69700,https://github.com/mrdoob/three.js\n",
    "libgdx,libgdx,18300,https://github.com/libgdx/libgdx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd0c6f8",
   "metadata": {},
   "source": [
    "## Scrape the list of topics from GitHub\n",
    "\n",
    "Explain how you'll do it\n",
    "\n",
    "- Usiing requests to download the page\n",
    "- Using bs4 to parse and extract innformation\n",
    "- Convert to a Pandas dataframe\n",
    "\n",
    "Let's write a function to download the page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed571a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cd9d03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_page():\n",
    "    # TODO - add comments\n",
    "    topics_url = 'https://github.com/topics'\n",
    "    response = requests.get(topics_url)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception('Fail to load page {}'.format(topics_url))\n",
    "    doc = BeautifulSoup(response.text,'html.parser')\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5a839a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = get_topic_page()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d9159b",
   "metadata": {},
   "source": [
    "Add some explination"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1a520b",
   "metadata": {},
   "source": [
    "Let's create some helper function to parse information from the page.\n",
    "\n",
    "\n",
    "\n",
    "To get topic titles, we can pick `p` tags with the `class`...\n",
    "\n",
    "![](https://i.imgur.com/nR2YFka.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c325ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_titles(doc):\n",
    "    topic_title_tags = doc.find_all('p',{'class':'f3 lh-condensed mb-0 mt-1 Link--primary'})\n",
    "    title_tags = []\n",
    "    for tags in topic_title_tags:\n",
    "        title_tags.append(tags.text.strip())\n",
    "    return title_tags\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8257291a",
   "metadata": {},
   "source": [
    "`get_topic_titles` can be used to get the list of titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1442f451",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = get_topic_titles(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c10cb4e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3eaa052",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3D', 'Ajax', 'Algorithm', 'Amp', 'Android']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5d5b28",
   "metadata": {},
   "source": [
    "Similarly we have defined funtions for descriptions and URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e18f0c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topics_desc(doc):\n",
    "    h3_selection_class = 'f3 color-text-secondary text-normal lh-condensed'\n",
    "    repo_tags = topic_doc.find_all('h3',{'class':h1_selection_class})\n",
    "    desc_tags = []\n",
    "    for tags in topic_desc_tags:\n",
    "        desc_tags.append(tags.text.strip())\n",
    "    return desc_tags\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e691425",
   "metadata": {},
   "source": [
    "TODO - example and explination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c901e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topics_urls(doc):\n",
    "    topic_link_tags = doc.find_all('a',{'class':'d-flex no-underline'})\n",
    "    topic_urls = []\n",
    "    base_url = 'https://github.com'\n",
    "    for i in range(0,len(topic_link_tags)):\n",
    "        topic_urls.append(\"https://github.com\" + topic_link_tags[i]['href'])\n",
    "    return topic_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fa261c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f090d077",
   "metadata": {},
   "source": [
    "Let's put this all togather into a single function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ff3caac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_topics():\n",
    "    topics_url = 'https://github.com/topics'\n",
    "    response = requests.get(topic_url)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception('Fail to load page {}'.format(topic_url))\n",
    "    doc = BeautifulSoup(response.text,'html.parser')\n",
    "    topics_dict = {\n",
    "        'title': get_topic_titles(doc),\n",
    "        'description': get_topics_desc(doc),\n",
    "        'url': get_topics_urls(doc)\n",
    "    }\n",
    "    return pd.DataFrame(topics_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52c5546",
   "metadata": {},
   "source": [
    "## Get the top repositories from the topic page\n",
    "\n",
    "TODO - explination and steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ae53030",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_page(topic_url):\n",
    "    # Download the page\n",
    "    response = requests.get(topic_url)\n",
    "    # Check successful response\n",
    "    if response.status_code != 200:\n",
    "        raise Exception('Fail to load page {}'.format(topic_url))\n",
    "    # Parse using BeautifulSoup\n",
    "    topic_doc = BeautifulSoup(response.text,'html.parser')\n",
    "    return topic_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8364342e",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = get_topic_page('https://github.com/topics/3d')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c05731",
   "metadata": {},
   "source": [
    "TODO - talk about the h3 tags "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c845e890",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_repo_info(h3_tags,star_tags):\n",
    "    # Returns all the required info about the repository\n",
    "    a_tags = h3_tags.find_all('a')\n",
    "    username = a_tags[0].text.strip()\n",
    "    repo_name = a_tags[1].text.strip()\n",
    "    repo_url =  base_url + a_tags[1]['href']\n",
    "    stars = parse_star_count(star_tags.text.strip())\n",
    "    return username,repo_name,stars,repo_url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3888ca8",
   "metadata": {},
   "source": [
    "TODO - show an example about get_repo_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d0e5a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_repos(topic_doc):\n",
    "    \n",
    "    # Get h3 tag containing repo title, repo url and username\n",
    "    h3_selection_class = 'f3 color-text-secondary text-normal lh-condensed'\n",
    "    repo_tags = topic_doc.find_all('h3',{'class':h1_selection_class})\n",
    "    # Get star tags\n",
    "    star_tags = topic_doc.find_all('a',{'class':'social-count float-none'})\n",
    "    \n",
    "    topic_repos_dict = {\n",
    "    'username':[],\n",
    "    'repo_name':[],\n",
    "    'stars':[],\n",
    "    'repo_url':[]\n",
    "}\n",
    "    \n",
    "    # Get repo info\n",
    "    for i in range(len(repo_tags)):\n",
    "        repo_info = get_repo_info(repo_tags[i],star_tags[i])\n",
    "        topic_repos_dict['username'].append(repo_info[0])\n",
    "        topic_repos_dict['repo_name'].append(repo_info[1])\n",
    "        topic_repos_dict['stars'].append(repo_info[2])\n",
    "        topic_repos_dict['repo_url'].append(repo_info[3])\n",
    "        \n",
    "        \n",
    "    return pd.DataFrame(topic_repos_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158e8dee",
   "metadata": {},
   "source": [
    "TODO - Show an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5625fce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_topic(topics_url, path):\n",
    "    if os.path.exists(path):\n",
    "        print(\"The file {} already exists. Skipping...\".format(path))\n",
    "        return\n",
    "    topic_df = get_topic_repos(get_topic_page(topic_url))\n",
    "    topic_df.to_csv(path, index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89c8ae2",
   "metadata": {},
   "source": [
    "TODO - Show an example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3891fe2",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fcdd04",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bbda52",
   "metadata": {},
   "source": [
    "## Putting it all togather\n",
    "\n",
    "- We have a functin to get the list of topics\n",
    "- We have function to create a CSV file for scraped repos for a tpoics page \n",
    "-  Let's create a function to put them togather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9faf1a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_topics_repos():\n",
    "    print('Scraping list of topics')\n",
    "    topics_df = scrape_topics()\n",
    "    \n",
    "    os.makedirs('data',exist_ok=True)\n",
    "    \n",
    "    for index, row in topics_df.iterrows():\n",
    "        print('Scraping top repositiories for \"{}\"'.format(row['title']))\n",
    "        scrape_topic(row['url'],'data/{}.csv'.format(row['title']) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec3803c",
   "metadata": {},
   "source": [
    "Let's run it to scrape the top repos for all the topics on the first page of the https://github.com/topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3bea57a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape_topics_repos()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1bb032",
   "metadata": {},
   "source": [
    "We can check that the CSVs were created properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ca97935a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read a CSV using pandas and show the data collected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff28615",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3e7292",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd726495",
   "metadata": {},
   "source": [
    "## References and Future Work\n",
    "\n",
    "Summary of what we did\n",
    "\n",
    "- We scrapped the github topics page in order to find out the topic 30 topics of the github topics\n",
    "- We got there names, the number of star they contain, there URL\n",
    "\n",
    "Refences to links you found useful\n",
    "\n",
    "- https://www.youtube.com/watch?v=RKsLLG-bzEY&t=555s\n",
    "- https://github.com/topics\n",
    "\n",
    "Ideas for future work\n",
    "\n",
    "- Based on the data we extracted, we can analyse the data using pandas dataframe and get the useful know the topics in github\n",
    "- We can also extract most of the data from differet websites in almost the same way\n",
    "- Filmography of Actors/Directors (Wikipedia)\n",
    "- Discography of an Artist (Wikipedia)\n",
    "- Dataset of Movies (TMDb)\n",
    "- Dataset of TV Shows (TMDb)\n",
    "- Collections of Popular Repositories (GitHub)\n",
    "- Dataset of Books (BooksToScrape)\n",
    "- Dataset of Quotes (QuotesToScrape)\n",
    "- Bibliography of an Author (Wikipedia)\n",
    "- Country Demographics (Wikipedia)\n",
    "- Stocks Prices (Yahoo Finance)\n",
    "- Create a Dataset of YouTube Videos (YouTube)\n",
    "- Songs Dataset (AZLyrics\n",
    "- Scrape a Popular Blog\n",
    "- Weekly Top Songs (Top 40 Weekly)\n",
    "- Video Games Dataset (Steam)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
